{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "elegant-inside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_optimizer in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: pytorch-ranger>=0.1.1 in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from torch_optimizer) (0.1.1)\n",
      "Requirement already satisfied: torch>=1.1.0 in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from torch_optimizer) (1.5.1+cu101)\n",
      "Requirement already satisfied: future in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from torch>=1.1.0->torch_optimizer) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\csera\\appdata\\roaming\\python\\python36\\site-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
      "Requirement already satisfied: adabelief_pytorch==0.2.0 in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from adabelief_pytorch==0.2.0) (1.5.1+cu101)\n",
      "Requirement already satisfied: tabulate>=0.7 in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from adabelief_pytorch==0.2.0) (0.8.7)\n",
      "Requirement already satisfied: colorama>=0.4.0 in c:\\users\\csera\\appdata\\roaming\\python\\python36\\site-packages (from adabelief_pytorch==0.2.0) (0.4.4)\n",
      "Requirement already satisfied: future in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from torch>=0.4.0->adabelief_pytorch==0.2.0) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\csera\\appdata\\roaming\\python\\python36\\site-packages (from torch>=0.4.0->adabelief_pytorch==0.2.0) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_optimizer\n",
    "!pip install adabelief_pytorch==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "southeast-artwork",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "tensor([[ 69.,  96.,  70.],\n",
      "        [102.,  43.,  37.]])\n",
      "tensor([[103., 119.],\n",
      "        [ 22.,  37.]])\n",
      "Parameter containing:\n",
      "tensor([[-0.2879,  0.0684,  0.3494],\n",
      "        [-0.4197, -0.5304, -0.4759]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2824, -0.2057], requires_grad=True)\n",
      "Epoch [10/1000], Loss: 7098.9224\n",
      "Epoch [20/1000], Loss: 36873.4609\n",
      "Epoch [30/1000], Loss: 34213.8672\n",
      "Epoch [40/1000], Loss: 5105.9722\n",
      "Epoch [50/1000], Loss: 29536.7715\n",
      "Epoch [60/1000], Loss: 16407.3145\n",
      "Epoch [70/1000], Loss: 25352.6484\n",
      "Epoch [80/1000], Loss: 13819.9072\n",
      "Epoch [90/1000], Loss: 6423.6152\n",
      "Epoch [100/1000], Loss: 5853.4258\n",
      "Epoch [110/1000], Loss: 18396.1855\n",
      "Epoch [120/1000], Loss: 12821.5547\n",
      "Epoch [130/1000], Loss: 11820.9697\n",
      "Epoch [140/1000], Loss: 14290.2715\n",
      "Epoch [150/1000], Loss: 9957.5967\n",
      "Epoch [160/1000], Loss: 6530.5293\n",
      "Epoch [170/1000], Loss: 1014.7029\n",
      "Epoch [180/1000], Loss: 5263.7339\n",
      "Epoch [190/1000], Loss: 4694.6387\n",
      "Epoch [200/1000], Loss: 1940.4554\n",
      "Epoch [210/1000], Loss: 7227.2720\n",
      "Epoch [220/1000], Loss: 3194.7251\n",
      "Epoch [230/1000], Loss: 4586.2671\n",
      "Epoch [240/1000], Loss: 1047.0845\n",
      "Epoch [250/1000], Loss: 312.8831\n",
      "Epoch [260/1000], Loss: 723.6167\n",
      "Epoch [270/1000], Loss: 1443.2043\n",
      "Epoch [280/1000], Loss: 467.0121\n",
      "Epoch [290/1000], Loss: 2642.6252\n",
      "Epoch [300/1000], Loss: 2260.5071\n",
      "Epoch [310/1000], Loss: 605.5038\n",
      "Epoch [320/1000], Loss: 469.1973\n",
      "Epoch [330/1000], Loss: 355.6834\n",
      "Epoch [340/1000], Loss: 1202.8456\n",
      "Epoch [350/1000], Loss: 42.6737\n",
      "Epoch [360/1000], Loss: 819.3926\n",
      "Epoch [370/1000], Loss: 89.5767\n",
      "Epoch [380/1000], Loss: 61.0350\n",
      "Epoch [390/1000], Loss: 523.2560\n",
      "Epoch [400/1000], Loss: 861.8895\n",
      "Epoch [410/1000], Loss: 354.6686\n",
      "Epoch [420/1000], Loss: 933.3623\n",
      "Epoch [430/1000], Loss: 942.1111\n",
      "Epoch [440/1000], Loss: 9.2958\n",
      "Epoch [450/1000], Loss: 219.9922\n",
      "Epoch [460/1000], Loss: 7.8212\n",
      "Epoch [470/1000], Loss: 194.0302\n",
      "Epoch [480/1000], Loss: 38.9509\n",
      "Epoch [490/1000], Loss: 10.6333\n",
      "Epoch [500/1000], Loss: 187.8323\n",
      "Epoch [510/1000], Loss: 35.7183\n",
      "Epoch [520/1000], Loss: 29.3599\n",
      "Epoch [530/1000], Loss: 173.2542\n",
      "Epoch [540/1000], Loss: 182.6479\n",
      "Epoch [550/1000], Loss: 170.8884\n",
      "Epoch [560/1000], Loss: 724.7641\n",
      "Epoch [570/1000], Loss: 26.6739\n",
      "Epoch [580/1000], Loss: 27.9984\n",
      "Epoch [590/1000], Loss: 668.4108\n",
      "Epoch [600/1000], Loss: 25.0045\n",
      "Epoch [610/1000], Loss: 102.7153\n",
      "Epoch [620/1000], Loss: 145.7762\n",
      "Epoch [630/1000], Loss: 22.9541\n",
      "Epoch [640/1000], Loss: 129.2850\n",
      "Epoch [650/1000], Loss: 69.0471\n",
      "Epoch [660/1000], Loss: 5.1265\n",
      "Epoch [670/1000], Loss: 69.3471\n",
      "Epoch [680/1000], Loss: 496.5639\n",
      "Epoch [690/1000], Loss: 476.0868\n",
      "Epoch [700/1000], Loss: 107.1629\n",
      "Epoch [710/1000], Loss: 438.3421\n",
      "Epoch [720/1000], Loss: 410.7703\n",
      "Epoch [730/1000], Loss: 394.4547\n",
      "Epoch [740/1000], Loss: 388.6181\n",
      "Epoch [750/1000], Loss: 106.2485\n",
      "Epoch [760/1000], Loss: 1.9148\n",
      "Epoch [770/1000], Loss: 12.1564\n",
      "Epoch [780/1000], Loss: 34.8672\n",
      "Epoch [790/1000], Loss: 12.5775\n",
      "Epoch [800/1000], Loss: 285.6338\n",
      "Epoch [810/1000], Loss: 281.2769\n",
      "Epoch [820/1000], Loss: 1.2270\n",
      "Epoch [830/1000], Loss: 10.4325\n",
      "Epoch [840/1000], Loss: 20.3132\n",
      "Epoch [850/1000], Loss: 12.7002\n",
      "Epoch [860/1000], Loss: 15.4603\n",
      "Epoch [870/1000], Loss: 10.9205\n",
      "Epoch [880/1000], Loss: 62.8924\n",
      "Epoch [890/1000], Loss: 206.6041\n",
      "Epoch [900/1000], Loss: 64.7158\n",
      "Epoch [910/1000], Loss: 59.8029\n",
      "Epoch [920/1000], Loss: 7.8602\n",
      "Epoch [930/1000], Loss: 162.7336\n",
      "Epoch [940/1000], Loss: 1.2071\n",
      "Epoch [950/1000], Loss: 50.4532\n",
      "Epoch [960/1000], Loss: 4.3597\n",
      "Epoch [970/1000], Loss: 132.9668\n",
      "Epoch [980/1000], Loss: 3.7736\n",
      "Epoch [990/1000], Loss: 121.2929\n",
      "Epoch [1000/1000], Loss: 5.1225\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 2  # should be << number of observations\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n",
    "    \n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "#Adam Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr = 1e-3, betas= (0.9, 0.99))\n",
    "\n",
    "fit(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "great-failing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.2711,  72.9354],\n",
       "        [ 82.4718,  99.9207],\n",
       "        [118.3351, 130.4749],\n",
       "        [ 21.1113,  51.6273],\n",
       "        [102.3900, 109.3802]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "atlantic-nickel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[53.8207, 70.5284]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[75, 63, 44.]]))  #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "devoted-horizon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "tensor([[ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.]])\n",
      "tensor([[ 81., 101.],\n",
      "        [119., 133.]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2352, -0.5464,  0.0079],\n",
      "        [-0.2062,  0.1731,  0.0788]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0715, -0.3384], requires_grad=True)\n",
      "Epoch [10/1000], Loss: 19636.9590\n",
      "Epoch [20/1000], Loss: 12469.0762\n",
      "Epoch [30/1000], Loss: 11149.3281\n",
      "Epoch [40/1000], Loss: 350.1048\n",
      "Epoch [50/1000], Loss: 236.5825\n",
      "Epoch [60/1000], Loss: 1787.3027\n",
      "Epoch [70/1000], Loss: 9540.7891\n",
      "Epoch [80/1000], Loss: 5756.1968\n",
      "Epoch [90/1000], Loss: 238.7309\n",
      "Epoch [100/1000], Loss: 346.4984\n",
      "Epoch [110/1000], Loss: 477.9740\n",
      "Epoch [120/1000], Loss: 639.5429\n",
      "Epoch [130/1000], Loss: 3738.6418\n",
      "Epoch [140/1000], Loss: 355.4647\n",
      "Epoch [150/1000], Loss: 1663.2118\n",
      "Epoch [160/1000], Loss: 2240.8328\n",
      "Epoch [170/1000], Loss: 1902.5951\n",
      "Epoch [180/1000], Loss: 9.5063\n",
      "Epoch [190/1000], Loss: 15.0794\n",
      "Epoch [200/1000], Loss: 709.0036\n",
      "Epoch [210/1000], Loss: 0.4279\n",
      "Epoch [220/1000], Loss: 581.7637\n",
      "Epoch [230/1000], Loss: 2058.4790\n",
      "Epoch [240/1000], Loss: 2049.4058\n",
      "Epoch [250/1000], Loss: 826.3856\n",
      "Epoch [260/1000], Loss: 760.1843\n",
      "Epoch [270/1000], Loss: 23.8166\n",
      "Epoch [280/1000], Loss: 63.2744\n",
      "Epoch [290/1000], Loss: 66.3783\n",
      "Epoch [300/1000], Loss: 76.5029\n",
      "Epoch [310/1000], Loss: 36.6494\n",
      "Epoch [320/1000], Loss: 1910.5275\n",
      "Epoch [330/1000], Loss: 36.4058\n",
      "Epoch [340/1000], Loss: 73.7556\n",
      "Epoch [350/1000], Loss: 546.8644\n",
      "Epoch [360/1000], Loss: 1788.0684\n",
      "Epoch [370/1000], Loss: 315.2548\n",
      "Epoch [380/1000], Loss: 70.2089\n",
      "Epoch [390/1000], Loss: 1706.3287\n",
      "Epoch [400/1000], Loss: 61.9485\n",
      "Epoch [410/1000], Loss: 495.8761\n",
      "Epoch [420/1000], Loss: 37.0679\n",
      "Epoch [430/1000], Loss: 268.7293\n",
      "Epoch [440/1000], Loss: 34.4419\n",
      "Epoch [450/1000], Loss: 60.1939\n",
      "Epoch [460/1000], Loss: 252.0009\n",
      "Epoch [470/1000], Loss: 34.5367\n",
      "Epoch [480/1000], Loss: 63.4171\n",
      "Epoch [490/1000], Loss: 407.9298\n",
      "Epoch [500/1000], Loss: 1405.2673\n",
      "Epoch [510/1000], Loss: 213.5230\n",
      "Epoch [520/1000], Loss: 1366.5554\n",
      "Epoch [530/1000], Loss: 215.1261\n",
      "Epoch [540/1000], Loss: 55.8010\n",
      "Epoch [550/1000], Loss: 56.7175\n",
      "Epoch [560/1000], Loss: 195.3290\n",
      "Epoch [570/1000], Loss: 337.1778\n",
      "Epoch [580/1000], Loss: 195.2068\n",
      "Epoch [590/1000], Loss: 347.2129\n",
      "Epoch [600/1000], Loss: 340.9834\n",
      "Epoch [610/1000], Loss: 178.5340\n",
      "Epoch [620/1000], Loss: 48.0739\n",
      "Epoch [630/1000], Loss: 1017.7249\n",
      "Epoch [640/1000], Loss: 24.6256\n",
      "Epoch [650/1000], Loss: 163.5994\n",
      "Epoch [660/1000], Loss: 282.2167\n",
      "Epoch [670/1000], Loss: 141.6502\n",
      "Epoch [680/1000], Loss: 918.9290\n",
      "Epoch [690/1000], Loss: 272.4437\n",
      "Epoch [700/1000], Loss: 260.6117\n",
      "Epoch [710/1000], Loss: 32.4964\n",
      "Epoch [720/1000], Loss: 848.9774\n",
      "Epoch [730/1000], Loss: 28.3983\n",
      "Epoch [740/1000], Loss: 29.6324\n",
      "Epoch [750/1000], Loss: 42.4880\n",
      "Epoch [760/1000], Loss: 29.0293\n",
      "Epoch [770/1000], Loss: 746.6277\n",
      "Epoch [780/1000], Loss: 120.8026\n",
      "Epoch [790/1000], Loss: 718.1097\n",
      "Epoch [800/1000], Loss: 205.6654\n",
      "Epoch [810/1000], Loss: 685.0543\n",
      "Epoch [820/1000], Loss: 104.7055\n",
      "Epoch [830/1000], Loss: 30.9426\n",
      "Epoch [840/1000], Loss: 18.7322\n",
      "Epoch [850/1000], Loss: 27.3687\n",
      "Epoch [860/1000], Loss: 29.0249\n",
      "Epoch [870/1000], Loss: 189.9556\n",
      "Epoch [880/1000], Loss: 574.9244\n",
      "Epoch [890/1000], Loss: 21.8144\n",
      "Epoch [900/1000], Loss: 18.6021\n",
      "Epoch [910/1000], Loss: 13.7875\n",
      "Epoch [920/1000], Loss: 482.0190\n",
      "Epoch [930/1000], Loss: 11.1284\n",
      "Epoch [940/1000], Loss: 464.8642\n",
      "Epoch [950/1000], Loss: 14.7010\n",
      "Epoch [960/1000], Loss: 19.3917\n",
      "Epoch [970/1000], Loss: 19.7108\n",
      "Epoch [980/1000], Loss: 131.7029\n",
      "Epoch [990/1000], Loss: 411.4516\n",
      "Epoch [1000/1000], Loss: 22.4075\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 2  # should be << number of observations\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n",
    "    \n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "# RMSprop Optimizer\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr = 1e-3, alpha = 0.9)\n",
    "\n",
    "fit(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "racial-premium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 62.5760,  70.2823],\n",
       "        [ 86.1894,  99.6639],\n",
       "        [104.2204, 134.7505],\n",
       "        [ 49.8504,  37.3517],\n",
       "        [ 91.8442, 117.1968]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unique-cedar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[61.4122, 67.1565]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[75, 63, 44.]]))  #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elder-operations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "tensor([[ 87., 134.,  58.],\n",
      "        [ 73.,  67.,  43.]])\n",
      "tensor([[119., 133.],\n",
      "        [ 56.,  70.]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2880, -0.3254, -0.0214],\n",
      "        [ 0.2467, -0.2687,  0.4600]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4249, 0.2777], requires_grad=True)\n",
      "Epoch [10/1000], Loss: 2456.9268\n",
      "Epoch [20/1000], Loss: 69.5563\n",
      "Epoch [30/1000], Loss: 54.6558\n",
      "Epoch [40/1000], Loss: 791.9073\n",
      "Epoch [50/1000], Loss: 539.6657\n",
      "Epoch [60/1000], Loss: 104.4114\n",
      "Epoch [70/1000], Loss: 14.4393\n",
      "Epoch [80/1000], Loss: 66.9806\n",
      "Epoch [90/1000], Loss: 1.9617\n",
      "Epoch [100/1000], Loss: 75.8572\n",
      "Epoch [110/1000], Loss: 0.1052\n",
      "Epoch [120/1000], Loss: 1.6111\n",
      "Epoch [130/1000], Loss: 29.3748\n",
      "Epoch [140/1000], Loss: 0.2482\n",
      "Epoch [150/1000], Loss: 1.6105\n",
      "Epoch [160/1000], Loss: 7.4663\n",
      "Epoch [170/1000], Loss: 5.6129\n",
      "Epoch [180/1000], Loss: 2.5126\n",
      "Epoch [190/1000], Loss: 1.6862\n",
      "Epoch [200/1000], Loss: 1.7487\n",
      "Epoch [210/1000], Loss: 0.4011\n",
      "Epoch [220/1000], Loss: 2.1838\n",
      "Epoch [230/1000], Loss: 0.2665\n",
      "Epoch [240/1000], Loss: 0.3852\n",
      "Epoch [250/1000], Loss: 0.4181\n",
      "Epoch [260/1000], Loss: 0.2094\n",
      "Epoch [270/1000], Loss: 8.5322\n",
      "Epoch [280/1000], Loss: 3.0687\n",
      "Epoch [290/1000], Loss: 0.7924\n",
      "Epoch [300/1000], Loss: 2.6525\n",
      "Epoch [310/1000], Loss: 1.7004\n",
      "Epoch [320/1000], Loss: 6.8555\n",
      "Epoch [330/1000], Loss: 2.4502\n",
      "Epoch [340/1000], Loss: 0.8819\n",
      "Epoch [350/1000], Loss: 0.1058\n",
      "Epoch [360/1000], Loss: 4.9250\n",
      "Epoch [370/1000], Loss: 0.1603\n",
      "Epoch [380/1000], Loss: 1.7154\n",
      "Epoch [390/1000], Loss: 4.8107\n",
      "Epoch [400/1000], Loss: 0.8891\n",
      "Epoch [410/1000], Loss: 0.1681\n",
      "Epoch [420/1000], Loss: 4.5056\n",
      "Epoch [430/1000], Loss: 0.1803\n",
      "Epoch [440/1000], Loss: 4.0006\n",
      "Epoch [450/1000], Loss: 0.1826\n",
      "Epoch [460/1000], Loss: 3.6740\n",
      "Epoch [470/1000], Loss: 0.9109\n",
      "Epoch [480/1000], Loss: 0.9255\n",
      "Epoch [490/1000], Loss: 0.4183\n",
      "Epoch [500/1000], Loss: 0.9401\n",
      "Epoch [510/1000], Loss: 2.9545\n",
      "Epoch [520/1000], Loss: 0.6459\n",
      "Epoch [530/1000], Loss: 0.8902\n",
      "Epoch [540/1000], Loss: 3.2277\n",
      "Epoch [550/1000], Loss: 0.1977\n",
      "Epoch [560/1000], Loss: 0.8789\n",
      "Epoch [570/1000], Loss: 2.7511\n",
      "Epoch [580/1000], Loss: 0.2716\n",
      "Epoch [590/1000], Loss: 0.3068\n",
      "Epoch [600/1000], Loss: 0.1797\n",
      "Epoch [610/1000], Loss: 0.9270\n",
      "Epoch [620/1000], Loss: 0.8334\n",
      "Epoch [630/1000], Loss: 0.9946\n",
      "Epoch [640/1000], Loss: 1.1579\n",
      "Epoch [650/1000], Loss: 0.4566\n",
      "Epoch [660/1000], Loss: 2.0965\n",
      "Epoch [670/1000], Loss: 0.8299\n",
      "Epoch [680/1000], Loss: 0.9427\n",
      "Epoch [690/1000], Loss: 0.1115\n",
      "Epoch [700/1000], Loss: 1.0510\n",
      "Epoch [710/1000], Loss: 0.6420\n",
      "Epoch [720/1000], Loss: 0.9305\n",
      "Epoch [730/1000], Loss: 0.3324\n",
      "Epoch [740/1000], Loss: 1.0883\n",
      "Epoch [750/1000], Loss: 0.5412\n",
      "Epoch [760/1000], Loss: 1.1080\n",
      "Epoch [770/1000], Loss: 0.9673\n",
      "Epoch [780/1000], Loss: 1.4675\n",
      "Epoch [790/1000], Loss: 0.8973\n",
      "Epoch [800/1000], Loss: 0.2933\n",
      "Epoch [810/1000], Loss: 1.6201\n",
      "Epoch [820/1000], Loss: 1.1053\n",
      "Epoch [830/1000], Loss: 0.2822\n",
      "Epoch [840/1000], Loss: 0.0285\n",
      "Epoch [850/1000], Loss: 0.9554\n",
      "Epoch [860/1000], Loss: 0.4612\n",
      "Epoch [870/1000], Loss: 0.7215\n",
      "Epoch [880/1000], Loss: 1.3359\n",
      "Epoch [890/1000], Loss: 0.3098\n",
      "Epoch [900/1000], Loss: 0.0158\n",
      "Epoch [910/1000], Loss: 0.3191\n",
      "Epoch [920/1000], Loss: 0.4765\n",
      "Epoch [930/1000], Loss: 0.7107\n",
      "Epoch [940/1000], Loss: 0.9107\n",
      "Epoch [950/1000], Loss: 0.6910\n",
      "Epoch [960/1000], Loss: 0.9971\n",
      "Epoch [970/1000], Loss: 0.9514\n",
      "Epoch [980/1000], Loss: 0.9704\n",
      "Epoch [990/1000], Loss: 1.1789\n",
      "Epoch [1000/1000], Loss: 1.0517\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 2  # should be << number of observations\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n",
    "    \n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "# SGD Optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "fit(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cubic-growth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.2345,  70.3424],\n",
       "        [ 82.0046, 100.5649],\n",
       "        [118.9157, 133.0267],\n",
       "        [ 21.1496,  37.0225],\n",
       "        [101.6213, 119.0134]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "known-electric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[53.6845, 67.4505]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[75, 63, 44.]]))  #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "driven-demand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "tensor([[91., 88., 64.],\n",
      "        [73., 67., 43.]])\n",
      "tensor([[ 81., 101.],\n",
      "        [ 56.,  70.]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.5733,  0.1819, -0.4031],\n",
      "        [-0.1938,  0.4370,  0.1081]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5077, 0.1739], requires_grad=True)\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Epoch [10/1000], Loss: 358.8843\n",
      "Epoch [20/1000], Loss: 980.1455\n",
      "Epoch [30/1000], Loss: 1162.7297\n",
      "Epoch [40/1000], Loss: 978.1852\n",
      "Epoch [50/1000], Loss: 80.4590\n",
      "Epoch [60/1000], Loss: 975.4134\n",
      "Epoch [70/1000], Loss: 79.8270\n",
      "Epoch [80/1000], Loss: 971.9753\n",
      "Epoch [90/1000], Loss: 1165.1903\n",
      "Epoch [100/1000], Loss: 1754.7004\n",
      "Epoch [110/1000], Loss: 1166.3842\n",
      "Epoch [120/1000], Loss: 964.0449\n",
      "Epoch [130/1000], Loss: 1747.5388\n",
      "Epoch [140/1000], Loss: 959.3124\n",
      "Epoch [150/1000], Loss: 1741.5925\n",
      "Epoch [160/1000], Loss: 342.4568\n",
      "Epoch [170/1000], Loss: 1170.4265\n",
      "Epoch [180/1000], Loss: 74.7446\n",
      "Epoch [190/1000], Loss: 1171.6901\n",
      "Epoch [200/1000], Loss: 73.6866\n",
      "Epoch [210/1000], Loss: 73.1236\n",
      "Epoch [220/1000], Loss: 1173.8446\n",
      "Epoch [230/1000], Loss: 1717.5315\n",
      "Epoch [240/1000], Loss: 1175.7076\n",
      "Epoch [250/1000], Loss: 326.9332\n",
      "Epoch [260/1000], Loss: 1706.8386\n",
      "Epoch [270/1000], Loss: 1178.4861\n",
      "Epoch [280/1000], Loss: 68.7989\n",
      "Epoch [290/1000], Loss: 1695.5710\n",
      "Epoch [300/1000], Loss: 67.4982\n",
      "Epoch [310/1000], Loss: 66.8494\n",
      "Epoch [320/1000], Loss: 312.9532\n",
      "Epoch [330/1000], Loss: 65.5425\n",
      "Epoch [340/1000], Loss: 1676.1771\n",
      "Epoch [350/1000], Loss: 895.3190\n",
      "Epoch [360/1000], Loss: 304.1923\n",
      "Epoch [370/1000], Loss: 888.1626\n",
      "Epoch [380/1000], Loss: 884.1980\n",
      "Epoch [390/1000], Loss: 297.4597\n",
      "Epoch [400/1000], Loss: 1650.5305\n",
      "Epoch [410/1000], Loss: 60.0310\n",
      "Epoch [420/1000], Loss: 1195.3032\n",
      "Epoch [430/1000], Loss: 1638.0972\n",
      "Epoch [440/1000], Loss: 1197.5334\n",
      "Epoch [450/1000], Loss: 284.6222\n",
      "Epoch [460/1000], Loss: 1199.6803\n",
      "Epoch [470/1000], Loss: 56.0843\n",
      "Epoch [480/1000], Loss: 278.2020\n",
      "Epoch [490/1000], Loss: 276.0013\n",
      "Epoch [500/1000], Loss: 54.1086\n",
      "Epoch [510/1000], Loss: 1205.5830\n",
      "Epoch [520/1000], Loss: 52.7877\n",
      "Epoch [530/1000], Loss: 267.4456\n",
      "Epoch [540/1000], Loss: 265.1267\n",
      "Epoch [550/1000], Loss: 1210.8226\n",
      "Epoch [560/1000], Loss: 1583.9548\n",
      "Epoch [570/1000], Loss: 815.9274\n",
      "Epoch [580/1000], Loss: 1214.5902\n",
      "Epoch [590/1000], Loss: 1570.7654\n",
      "Epoch [600/1000], Loss: 1566.2124\n",
      "Epoch [610/1000], Loss: 249.6841\n",
      "Epoch [620/1000], Loss: 247.6132\n",
      "Epoch [630/1000], Loss: 45.7909\n",
      "Epoch [640/1000], Loss: 243.0762\n",
      "Epoch [650/1000], Loss: 44.5479\n",
      "Epoch [660/1000], Loss: 782.3906\n",
      "Epoch [670/1000], Loss: 43.3554\n",
      "Epoch [680/1000], Loss: 774.7467\n",
      "Epoch [690/1000], Loss: 1230.6886\n",
      "Epoch [700/1000], Loss: 1521.3104\n",
      "Epoch [710/1000], Loss: 1516.3773\n",
      "Epoch [720/1000], Loss: 1235.3425\n",
      "Epoch [730/1000], Loss: 756.2081\n",
      "Epoch [740/1000], Loss: 752.5210\n",
      "Epoch [750/1000], Loss: 748.9745\n",
      "Epoch [760/1000], Loss: 38.2445\n",
      "Epoch [770/1000], Loss: 1490.6843\n",
      "Epoch [780/1000], Loss: 738.1960\n",
      "Epoch [790/1000], Loss: 36.6277\n",
      "Epoch [800/1000], Loss: 36.0996\n",
      "Epoch [810/1000], Loss: 1249.3207\n",
      "Epoch [820/1000], Loss: 723.5435\n",
      "Epoch [830/1000], Loss: 34.5618\n",
      "Epoch [840/1000], Loss: 716.1403\n",
      "Epoch [850/1000], Loss: 712.5725\n",
      "Epoch [860/1000], Loss: 1257.4587\n",
      "Epoch [870/1000], Loss: 194.8165\n",
      "Epoch [880/1000], Loss: 32.1562\n",
      "Epoch [890/1000], Loss: 1261.9962\n",
      "Epoch [900/1000], Loss: 1263.3093\n",
      "Epoch [910/1000], Loss: 692.3295\n",
      "Epoch [920/1000], Loss: 689.1548\n",
      "Epoch [930/1000], Loss: 1421.8788\n",
      "Epoch [940/1000], Loss: 182.2256\n",
      "Epoch [950/1000], Loss: 1414.3086\n",
      "Epoch [960/1000], Loss: 1271.9951\n",
      "Epoch [970/1000], Loss: 176.8667\n",
      "Epoch [980/1000], Loss: 27.9721\n",
      "Epoch [990/1000], Loss: 173.3840\n",
      "Epoch [1000/1000], Loss: 171.6740\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 2  # should be << number of observations\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n",
    "    \n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "#Adabelief Optimizer\n",
    "#opt = optim.AdaBelief(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8)\n",
    "opt = AdaBelief(model.parameters(), lr=1e-5, eps=1e-16, betas=(0.9, 0.999), print_change_log = False)\n",
    "\n",
    "fit(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opponent-machine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 59.0362,  63.2777],\n",
       "        [ 71.8220,  84.9066],\n",
       "        [ 87.1228, 115.3324],\n",
       "        [ 71.7010,  46.2558],\n",
       "        [ 58.1952,  91.0545]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "running-tennessee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[58.6524, 60.8542]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[75, 63, 44.]]))  #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-clearance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
